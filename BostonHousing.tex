\documentclass[12 pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage[mathscr]{euscript}
\usepackage{youngtab}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumerate}
\newcommand{\W}{\mathcal{W}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\pf}{\tilde{\phi}_N}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\su}{\mathfrak{su}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\usp}{\mathfrak{usp}}
\newcommand{\h}{\mathfrak{h}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\mon}{\OO_{\vec{n}}(a)}
\newcommand{\I}{\mathcal{I}}
\numberwithin{equation}{section}
\begin{document}

\title{Boston Housing Prices \\ \footnotesize{Udacity Machine Learning Engineer \\ Nanodegree Program: Project 1}}
\author{Anderson Daniel Trimm}
\date{\today}
\maketitle
\begin{abstract}
In this report, we present two analyses using the Boston housing dataset: First, we perform a statistical analysis of the dataset using NumPy. Following this analysis, we optimize a decision tree regression algorithm and use it to predict the value of a house using scikit-learn. 
\end{abstract}
\section{Introduction}
\section{Statistical Analysis of the Boston Housing Dataset}
In this section we compute basic statistics of the Boston Housing dataset using NumPy. To begin, we need to import the Boston Housing dataset as well as Numpy:

\begin{verbatim}
	import numpy as np
	from sklearn import datasets
\end{verbatim}

We can now define the following function to load the Boston dataset:

\begin{verbatim}
	def load_data():
    """Load the Boston dataset."""

    boston = datasets.load_boston()
    return boston
\end{verbatim}
After loading the Boston dataset, we can look at its attributes \texttt{boston.data} and \texttt{boston.target} to access the features and housing prices. The attribute \texttt{boston.data} gives a two-dimensional ndarray, where each row is the list of features for a given house. The attribute \texttt{boston.target} gives a one-dimensional ndarray of the housing prices. The total number of houses is therefore just the length of the ndarray boston.target:

\begin{verbatim}
	>>> np.shape(boston.target)
(506,)
\end{verbatim}
which we see is 506. The number of features per house then is the row length of the ndarray boston.data, which is the one-eth entry of
\begin{verbatim}
	>>> np.shape(boston.data)
(506, 13)
\end{verbatim}
which is 13. We can encapsulate these in functions as 
\begin{verbatim}
		def size_of_data(city_data):
    number_of_houses = np.shape(city_data.data)[0]
    return number_of_houses
\end{verbatim}
\begin{verbatim}
	def number_of_features(city_data):
    number_of_features = np.shape(city_data.data)[1]
    return number_of_features
\end{verbatim}

To compute the minimum, maximum, mean, and meadian price and the standard deviation, we simply use the methods \texttt{np.min(boston.target)},\texttt{np.max(boston.target)}, \texttt{np.mean(boston.target)}, \texttt{np.meadian(boston.target)}, and \texttt{np.std(boston.target)}, respectively. We find
\begin{verbatim}
	>>> np.min(boston.target)
5.0
>>> np.max(boston.target)
50.0
>>> np.mean(boston.target)
22.532806324110677
>>> np.median(boston.target)
21.199999999999999
>>> np.std(boston.target)
9.1880115452782025
\end{verbatim}
As before, we can encapsulate these in functions as
\begin{verbatim}
	def get_min_price(city_data):
    min_price = np.min(city_data.target)
    return min_price
\end{verbatim}
\begin{verbatim}
	def get_max_price(city_data):
    max_price = np.max(city_data.target)
    return max_price
\end{verbatim}
\begin{verbatim}
def get_mean_price(city_data):
    mean_price = np.mean(city_data.target)
    return mean_price
\end{verbatim}
\begin{verbatim}
	def get_median_price(city_data):
    median_price = np.median(city_data.target)
    return median_price
\end{verbatim}
\begin{verbatim}
	def get_standard_deviation(city_data):
    standard_deviation = np.std(city_data.target)
    return standard_deviation
\end{verbatim}

\section{Predicting Housing Prices}
In this section we... $\clubsuit$ FILL IN SOME INFO ABOUT THE DECISION TREE REGRESSOR
\subsection{Evaluating Model Performance}
Since we would like to be able to evaluate our model's ability to predict housing prices given new, unseen data, we begin by splitting the Boston dataset into a training and testing set. By holding out the testing data and allowing our model to learn only on the training set, we leave ourselves an independent set of data we can use to verify that our model can generalize well to unseen data. If we trained on the \emph{entire} dataset, we would have no way to evaluate how well our model can predict the housing price of new data points.

To split the dataset, we first import \texttt{train\_test\_split} from the \texttt{sklearn.cross\_validation} module:
\begin{verbatim}
	from sklearn.cross_validation import train_test_split
\end{verbatim}
and define the function $\clubsuit$ FIX TEXT RUNOFF
\begin{verbatim}
	def split_data(city_data):
    # Get the features and labels from the Boston housing data
    X, y = city_data.data, city_data.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)
    return X_train, y_train, X_test, y_test
\end{verbatim}
Choosing \texttt{test\_size$=$0.30} splits the data into 30 \% testing data and 70 \% training data, while \texttt{random\_state$=$42} is the (pseudo-)random number generator state used for random sampling (which is set arbitrarily to 42).

Next, we will train our model on the training set and compare with its performance on the testing set. To do so, we first need to choose an appropriate performance metric. Looking at the list of regression performance metrics on sklearn, we find four options:
\begin{verbatim} 	 
‘mean_absolute_error’ 
‘mean_squared_error’		 
‘median_absolute_error’		 
‘r2’
\end{verbatim}
Of these, one could argue that the \texttt{mean\_squared\_error} is the most appropriate performance metric for our algorithm, as the sklearn decision tree regressor already by default uses mean squared error to measure the quality of each split ($\clubsuit $ IS THIS REALLY A GOOD REASON?). Additionally, it has the desired properties that larger deviations from the true labels are penalized more heavily as well as being everywhere differentiable, so that one can compute the maximum and minimum errors using calculus, while none of the other choices share all of these properties. In the following, we will therefore use mean squared error as the performance metric for our model ($\clubsuit$ BE MORE SPECIFIC ABOUT WHAT PERFORMANCE METRIC IS ANALYZING). 

$\clubsuit$ INSERT DESCRIPTION OF LEARNING CURVES AND MODEL COMPLEXITY GRAPH.

($\clubsuit$ THIS PART ON CROSS VALIDATION WITH GRIDSEARCH.) However, we still have a parameter in our model - namely the depth of the decision tree, which we need to optimize. We will do this using grid search over a range of possible values for this parameter. Grid search will run our algorithm over the training and testing data for each of these parameters, and we can compare We first import \texttt{grid\_search} from \texttt{sklearn}:
\begin{verbatim}
	from sklearn import grid_search
\end{verbatim}
\end{document}