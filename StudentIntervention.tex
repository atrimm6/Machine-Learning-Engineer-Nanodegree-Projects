\documentclass[12 pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage[mathscr]{euscript}
\usepackage{youngtab}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{listings}
\newcommand{\W}{\mathcal{W}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\pf}{\tilde{\phi}_N}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\su}{\mathfrak{su}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\usp}{\mathfrak{usp}}
\newcommand{\h}{\mathfrak{h}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\mon}{\OO_{\vec{n}}(a)}
\newcommand{\I}{\mathcal{I}}
\numberwithin{equation}{section}
\begin{document}

\title{Student Intervention System \\ \footnotesize{Udacity Machine Learning Engineer \\ Nanodegree Program: Project 2}}
\author{Anderson Daniel Trimm}
\date{\today}
\maketitle

\section{Classification vs Regression}
Our model predicts which students will pass or fail their high school final exam, and therefore need an intervention. Since we are predicting \emph{discrete} labels (in this case, binary), this is a \emph{classification} problem, as opposed to a \emph{regression} problem, in which we would be predicting \emph{continuous} labels.
\section{Exploring the Data}
In this section, we use the code in the accompanying ipython notebook to identify important characteristics of the dataset which will influence our prediction. Running the code, we find

\begin{verbatim}	
Total number of students: 395
Number of students who passed: 265
Number of students who failed: 130
Number of features: 31
Graduation rate of the class: 0.67%
\end{verbatim}

\section{Training and Evaluating the Models}
\subsection{KNN Classifier}
The $k$-nearest neighbor classifier finds the $k$ training samples closest in distance to the query point, and predicts the label from these. \\
\\
Pros:
\begin{itemize}
	\item Fast training time - it simply remembers all the training points
	\item Being non-parametric, it can be useful in classification problems where the decision boundary is very irregular
\end{itemize}

Cons:
\begin{itemize}
	\item Potentially long training time - it has to search through the dataset to find the $k$ nearest neighbors
	\item Uses a lot of CPU memory, since it has to store the dataset (as opposed to a parametric model, which throws away the dataset after learning the parameters)
\end{itemize}

Despite the memory cost, since our dataset is not too large, kNN is a reasonable classifier to try.

\begin{center}
  \begin{tabular}{| c | c | c | c | }
    \hline
    & 100 & 200 & 300 \\ \hline
    Training time (sec) & & & \\ \hline
    Prediction time (sec) & & & \\ \hline 
    F1 score for training set & & & \\ \hline
    F1 score for test set & & & \\ \hline
  \end{tabular}
\end{center}

\subsection{Gaussian Naive Bayes Classifier}
Naive Bayes is a learning algorithm based on applying Bayes' theorem with the assumption that every pair of features are independent (hence the use of ``naive"). Here, we use the Gaussian naive Bayes classifier, which assumes the likelihood of each feature is Gaussian.\\
\\
$\clubsuit$ FINISH WATCHING VIDEOS AND FILL IN...\\
Pros:
\begin{itemize}
	\item 
\end{itemize}

Cons:
\begin{itemize}
	\item 
\end{itemize}

\begin{center}
  \begin{tabular}{| c | c | c | c | }
    \hline
    & 100 & 200 & 300 \\ \hline
    Training time (sec) & & & \\ \hline
    Prediction time (sec) & & & \\ \hline 
    F1 score for training set & & & \\ \hline
    F1 score for test set & & & \\ \hline
  \end{tabular}
\end{center}

\subsection{Support Vector Machine Classifier}
Pros:
\begin{itemize}
	\item Effective in high dimensional spaces
	\item Can still be effective in cases where the number of dimensions is greater than the number of samples
	\item Uses only a subset of training points in the decision function (the ``support vectors"), so it is memory efficient
	\item Versatile, since different kernel functions can be specified for the decision function
\end{itemize}
Cons:
\begin{itemize}
	\item If the number of features is much greater than the number of samples, it is likely to perform poorly
	\item Does not directly provide probability estimates; these are calculated using a memory expensive five-fold cross-validation
\end{itemize}

\begin{center}
  \begin{tabular}{| c | c | c | c | }
    \hline
    & 100 & 200 & 300 \\ \hline
    Training time (sec) & & & \\ \hline
    Prediction time (sec) & & & \\ \hline 
    F1 score for training set & & & \\ \hline
    F1 score for test set & & & \\ \hline
  \end{tabular}
\end{center}

\end{document}