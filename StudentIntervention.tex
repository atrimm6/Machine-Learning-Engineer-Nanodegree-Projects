\documentclass[12 pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{fullpage}
\usepackage[mathscr]{euscript}
\usepackage{youngtab}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{listings}
\newcommand{\W}{\mathcal{W}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\pf}{\tilde{\phi}_N}
\newcommand{\g}{\mathfrak{g}}
\newcommand{\su}{\mathfrak{su}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\usp}{\mathfrak{usp}}
\newcommand{\h}{\mathfrak{h}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\mon}{\OO_{\vec{n}}(a)}
\newcommand{\I}{\mathcal{I}}
\numberwithin{equation}{section}
\begin{document}

\title{Student Intervention System \\ \footnotesize{Udacity Machine Learning Engineer \\ Nanodegree Program: Project 2}}
\author{Anderson Daniel Trimm}
\date{\today}
\maketitle

\section{Classification vs Regression}
Our model predicts which students will pass or fail their high school final exam, and therefore need an intervention. Since we are predicting \emph{discrete} labels (in this case, binary), this is a \emph{classification} problem, as opposed to a \emph{regression} problem, in which we would be predicting \emph{continuous} labels.
\section{Exploring the Data}
In this section, we use the code in the accompanying ipython notebook to identify important characteristics of the dataset which will influence our prediction. Running the code, we find

\begin{verbatim}	
Total number of students: 395
Number of students who passed: 265
Number of students who failed: 130
Number of features: 31
Graduation rate of the class: 0.67%
\end{verbatim}

\section{Training and Evaluating the Models}
\subsection{KNN Classifier}
The $k$-nearest neighbor classifier finds the $k$ training samples closest in distance to the query point, and predicts the label from these. \\
\\
Pros:
\begin{itemize}
	\item Fast training time - it simply remembers all the training points
	\item Being non-parametric, it can be useful in classification problems where the decision boundary is very irregular
\end{itemize}

Cons:
\begin{itemize}
	\item Potentially long training time - it has to search through the dataset to find the $k$ nearest neighbors
	\item Uses a lot of CPU memory, since it has to store the dataset (as opposed to a parametric model, which throws away the dataset after learning the parameters)
\end{itemize}

Despite the memory cost, since our dataset is not too large, kNN is a reasonable classifier to try.
\subsection{Gaussian Naive Bayes Classifier}
Naive Bayes is a learning algorithm based on applying Bayes' theorem with the assumption that every pair of features are independent (hence the ``naive"). Here, we use the Gaussian naive Bayes classifier, which assumes the likelihood of each feature is Gaussian.\\
\\
Pros:
\begin{itemize}
	\item 
\end{itemize}

Cons:
\begin{itemize}
	\item 
\end{itemize}

\subsection{Support Vector Machine Classifier}

\end{document}